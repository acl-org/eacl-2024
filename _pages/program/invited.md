---
title: Invited Speakers
hide_title: true
layout: single
permalink: /program/invited/
sidebar:
  nav: program
toc: true
toc_only: false
toc_sticky: true
toc_icon: "cog" 
---
<h1>Invited Speakers</h1>

<h2>Keynote: <a href="https://homepages.inf.ed.ac.uk/mlap/" target="_blank">Mirella Lapata</a></h2><br/>
<img src="/assets/images/mirella.png" alt="Mirella Lapata">

<b>Title:</b> Prompting is *not* all you need! Or why Structure and Representations still matter in NLP <br/>

<b>Date:</b> Wednesday 20/03, 14:45-15:45 <br/>

<b>Abstract:</b> Recent years have witnessed the rise of increasingly larger and more sophisticated language models (LMs) capable of performing every task imaginable, sometimes at (super)human level. In this talk, I will argue that there is still space for specialist models in today's NLP landscape. Such models can be dramatically more efficient, inclusive, and explainable. I will focus on two examples, opinion summarization and crosslingual semantic parsing and show how these two seemingly unrelated tasks can be addressed by explicitly learning task-specific representations. I will show how such representations can be further structured to allow search and retrieval, evidence-based generation, and cross-lingual alignment. Finally, I will  discuss why we need to to use LLMs for what they are good at and remove the need for them to do things that can be done much better by smaller models.
<br/>

<b>Speaker Bio:</b> Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh. Her research focuses on getting computers to understand, reason with, and generate natural language. She is the first recipient (2009) of the British Computer Society and Information Retrieval Specialist Group (BCS/IRSG) Karen Sparck Jones award and a Fellow of the Royal Society of Edinburgh, the ACL, and Academia Europaea. Mirella has also received best paper awards in leading NLP conferences and has served on the editorial boards of the Journal of Artificial Intelligence Research, the Transactions of the ACL, and Computational Linguistics. She was president of SIGDAT (the group that organizes EMNLP) in 2018. She has been awarded an ERC consolidator grant, a Royal Society Wolfson Research Merit Award, and a UKRI Turing AI World-Leading Researcher Fellowship.
<br/>


<h2>Keynote: <a href="https://www.cis.lmu.de/schuetze/" target="_blank">Hinrich Schütze</a></h2><br/>
<img src="/assets/images/hinrich.png" alt="Hinrich Schütze">

<b>Title:</b> Quality Data for LLMs: Challenges and Opportunities for NLP <br/>

<b>Date:</b> Tuesday 19/03, 09:00-10:00 <br/>

<b>Abstract:</b> That the recent LLM breakthroughs are solely due to scaling is a myth. Many difficult research problems had to be solved to make models like GPT4 and Mixtral possible. One of those difficult research problems is data quality. Data quality is a great challenge for NLP researchers with many opportunities for innovation and impact on current generative AI developments. I will focus on two examples in my talk: quality data for training a highly multilingual language model and quality data for instruction tuning.
<br/>

<b>Speaker Bio:</b> Hinrich Schuetze is Professor at the Center for Information
and Language Processing at LMU Munich.  His lab is engaged in research on multilinguality, representation learning and linguistic analysis of NLP models.  His research has been funded by NSF, the German National Science Foundation and the European Research Council (ERC Advanced Grant), inter alia. Hinrich is coauthor of two well-known textbooks (Foundations of Statistical Natural Language Processing and Introduction to Information Retrieval), a fellow of HessianAI, ELLIS (the European Laboratory for Learning and Intelligent Systems) and ACL (Association for Computational Linguistics) and (co-)awardee of several best paper awards and the ACL 2023 25-year test of time award.

<h2>Karen Spärck Jones Award Lecture: <a href="https://www.cs.virginia.edu/~hw5x/" target="_blank">Hongning Wang</a></h2><br/>
<img src="/assets/images/hongning.jpg" alt="Hongning Wang">

<b>Title:</b> Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict? <br/>

<b>Date:</b> Monday 18/03, 09:30-10:30 <br/>

<b>Abstract:</b> The advent of generative AI technology produces transformative impact on the content creation landscape, offering alternative approaches to produce diverse, good-quality content across media, thereby reshaping the ecosystems of online content creation and publishing, but also raising concerns about market over-saturation and the potential marginalization of human creativity. Our recent work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and generative AI. Our theory and simulations suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in generative AI.
<br/>

<b>Speaker Bio:</b> Dr. Hongning Wang is now an associate professor at the Department of Computer Science and Technology at Tsinghua University. Prior to that, he was the Copenhaver Associate Professor in the Department of Computer Science at the University of Virginia. He received his PhD degree in computer science at the University of Illinois at Champaign-Urbana in 2014. His research generally lies in the intersection among machine learning and information retrieval, with a special focus on sequential decision optimization and computational user modeling. His work has generated over 100 research papers in top venues in data mining and information retrieval areas. He is a recipient of 2016 National Science Foundation CAREER Award, 2020 Google Faculty Research Award, and SIGIR’2019 Best Paper Award.
