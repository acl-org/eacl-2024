---
title: Invited Speakers
hide_title: true
layout: single
permalink: /program/invited/
sidebar:
  nav: program
toc: true
toc_only: false
toc_sticky: true
toc_icon: "cog" 
---
<h1>Invited Speakers</h1>

<h2><a href="https://homepages.inf.ed.ac.uk/mlap/" target="_blank">Mirella Lapata</a></h2><br/>
<img src="/assets/images/mirella.png" alt="Mirella Lapata">

<b>Title:</b> Prompting is *not* all you need! Or why Structure and Representations still matter in NLP <br/>

<b>Date:</b> <br/>

<b>Abstract:</b> Recent years have witnessed the rise of increasingly larger and more sophisticated language models (LMs) capable of performing every task imaginable, sometimes at (super)human level. In this talk, I will argue that there is still space for specialist models in today's NLP landscape. Such models can be dramatically more efficient, inclusive, and explainable. I will focus on two examples, opinion summarization and crosslingual semantic parsing and show how these two seemingly unrelated tasks can be addressed by explicitly learning task-specific representations. I will show how such representations can be further structured to allow search and retrieval, evidence-based generation, and cross-lingual alignment. Finally, I will  discuss why we need to to use LLMs for what they are good at and remove the need for them to do things that can be done much better by smaller models.
<br/>

<b>Speaker Bio:</b> Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh. Her research focuses on getting computers to understand, reason with, and generate natural language. She is the first recipient (2009) of the British Computer Society and Information Retrieval Specialist Group (BCS/IRSG) Karen Sparck Jones award and a Fellow of the Royal Society of Edinburgh, the ACL, and Academia Europaea. Mirella has also received best paper awards in leading NLP conferences and has served on the editorial boards of the Journal of Artificial Intelligence Research, the Transactions of the ACL, and Computational Linguistics. She was president of SIGDAT (the group that organizes EMNLP) in 2018. She has been awarded an ERC consolidator grant, a Royal Society Wolfson Research Merit Award, and a UKRI Turing AI World-Leading Researcher Fellowship.
<br/>


<h2><a href="https://www.cis.lmu.de/schuetze/" target="_blank">Hinrich Schütze</a></h2><br/>
<img src="/assets/images/hinrich.png" alt="Hinrich Schütze">

<b>Title:</b> Quality Data for LLMs: Challenges and Opportunities for NLP <br/>

<b>Date:</b> <br/>

<b>Abstract:</b> That the recent LLM breakthroughs are solely due to scaling is a myth. Many difficult research problems had to be solved to make models like GPT4 and Mixtral possible. One of those difficult research problems is data quality. Data quality is a great challenge for NLP researchers with many opportunities for innovation and impact on current generative AI developments. I will focus on two examples in my talk: quality data for training a highly multilingual language model and quality data for instruction tuning.
<br/>

<b>Speaker Bio:</b> Hinrich Schuetze is Professor at the Center for Information
and Language Processing at LMU Munich.  His lab is engaged in research on multilinguality, representation learning and linguistic analysis of NLP models.  His research has been funded by NSF, the German National Science Foundation and the European Research Council (ERC Advanced Grant), inter alia. Hinrich is coauthor of two well-known textbooks (Foundations of Statistical Natural Language Processing and Introduction to Information Retrieval), a fellow of HessianAI, ELLIS (the European Laboratory for Learning and Intelligent Systems) and ACL (Association for Computational Linguistics) and (co-)awardee of several best paper awards and the ACL 2023 25-year test of time award.
